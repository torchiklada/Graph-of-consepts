{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2zFz3J-LWeF"
      },
      "source": [
        "# **Импорт библиотек**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN-UBYor-Bfc"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5yqfEy4K0F8"
      },
      "outputs": [],
      "source": [
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.util import compile_infix_regex\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KYGR586MV5T"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOBpGNwDNCFg"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH219_e1wP4R"
      },
      "outputs": [],
      "source": [
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2nf2utZ3lVN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I47Nk8CO8LLZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE8JKVQvMZ7l"
      },
      "source": [
        "# **Инициализация функций**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP9X6o7YMfdA"
      },
      "outputs": [],
      "source": [
        "# лемматизация\n",
        "def lemma(word):\n",
        "    doc = nlp(word)\n",
        "\n",
        "    l = doc[0].lemma_\n",
        "    return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKDhkw6ZMmA_"
      },
      "outputs": [],
      "source": [
        "# удаление одиночных вершин\n",
        "def remove_isolated_nodes(graph):\n",
        "    isolated_nodes = [node for node in graph.nodes() if graph.degree(node) == 0]\n",
        "    graph.remove_nodes_from(isolated_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxBEjvnfb87d"
      },
      "outputs": [],
      "source": [
        "# поиск нужного эмбеддинга\n",
        "def find_embedding(word, bert_text):\n",
        "    for elem in bert_text:\n",
        "      if elem[0] == word:\n",
        "        return elem[1]\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B58bot7ofCnB"
      },
      "outputs": [],
      "source": [
        "# получение bert-эмбеддинга для каждого предложения\n",
        "def get_bert_vectors(text):\n",
        "\n",
        "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "  tokenized_text = tokenizer.tokenize(marked_text)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  segments_ids = [1] * len(tokenized_text)\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(tokens_tensor, segments_tensors)\n",
        "      hidden_states = outputs[2]\n",
        "\n",
        "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "  token_embeddings = token_embeddings.permute(1, 0, 2)\n",
        "\n",
        "  token_vecs_sum = []\n",
        "\n",
        "  for token in token_embeddings:\n",
        "      sum_vec = torch.sum(token[-4:], dim=0)\n",
        "      token_vecs_sum.append(sum_vec)\n",
        "\n",
        "  word_vectors = []\n",
        "  current_word = \"\"\n",
        "  current_vector = None\n",
        "\n",
        "  for i, token_str in enumerate(tokenized_text):\n",
        "      if token_str.startswith(\"##\"):\n",
        "          if current_vector is None:\n",
        "              current_vector = token_vecs_sum[i]\n",
        "              current_word = token_str\n",
        "          else:\n",
        "              current_vector += token_vecs_sum[i]\n",
        "              current_word += token_str[2:]\n",
        "      else:\n",
        "          if current_vector is not None:\n",
        "              word_vectors.append((current_word, current_vector.numpy()))\n",
        "          current_word = token_str\n",
        "          current_vector = token_vecs_sum[i]\n",
        "\n",
        "  if current_vector is not None:\n",
        "      word_vectors.append((current_word, current_vector.numpy()))\n",
        "\n",
        "  return word_vectors[1:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA9O7Mm4MoXe"
      },
      "outputs": [],
      "source": [
        "# создание графа униграмм\n",
        "def create_dependency_graph(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    verb_noun_dict = {}\n",
        "    embeddings = {}\n",
        "\n",
        "    for sent in doc.sents:\n",
        "      embs = get_bert_vectors(sent.text)\n",
        "\n",
        "      for token in sent:\n",
        "          if token.pos_ == 'VERB':\n",
        "              verb = (token.text, token.i)\n",
        "              if verb not in verb_noun_dict:\n",
        "                  verb_noun_dict[verb] = set()\n",
        "\n",
        "              for child in token.children:\n",
        "                  if child.pos_ == 'NOUN' and child.text.isalpha():\n",
        "                      noun = (child.text, child.i)\n",
        "                      embeddings[noun] = find_embedding(noun[0], embs)\n",
        "                      verb_noun_dict[verb].add(noun)\n",
        "\n",
        "    for verb, nouns in verb_noun_dict.items():\n",
        "        for noun1 in nouns:\n",
        "              G.add_node(noun1)\n",
        "              for noun2 in nouns:\n",
        "                    if noun1 != noun2:\n",
        "                        G.add_node(noun2)\n",
        "                        if noun1[1] < noun2[1]:\n",
        "                          G.add_edge(noun1, noun2, label=verb[0])\n",
        "                        else:\n",
        "                          G.add_edge(noun2, noun1, label=verb[0])\n",
        "\n",
        "    return G, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_OmVLqTK0mS"
      },
      "outputs": [],
      "source": [
        "# создание графа биграмм\n",
        "def create_dependency_graph_bi(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    verb_bigram_dict = {}\n",
        "    embeddings = {}\n",
        "\n",
        "    for sent in doc.sents:\n",
        "\n",
        "      embs = get_bert_vectors(sent.text)\n",
        "\n",
        "      for token in sent:\n",
        "          if token.pos_ == 'VERB':\n",
        "              verb = (token.text, token.i)\n",
        "              if verb not in verb_bigram_dict:\n",
        "                  verb_bigram_dict[verb] = set()\n",
        "\n",
        "              for child in token.children:\n",
        "                  if child.pos_ == 'NOUN' and child.text.isalpha() and len(child.text) > 1:\n",
        "                      noun = (child.text, child.i)\n",
        "                      embeddings_noun = find_embedding(noun[0], embs)\n",
        "\n",
        "                      for grandchild in child.children:\n",
        "                        if grandchild.text.isalpha() and grandchild.pos_ not in ['VERB', 'CONJ', 'DET', 'ADP'] and len(grandchild.text) > 1:\n",
        "                            dependent_word = (grandchild.text, grandchild.i)\n",
        "                            embeddings_dependent_word = find_embedding(dependent_word[0], embs)\n",
        "\n",
        "                            bigram = tuple(sorted([noun, dependent_word], key=lambda x: x[1]))\n",
        "                            embeddings[bigram] = embeddings_noun + embeddings_dependent_word\n",
        "                            verb_bigram_dict[verb].add(bigram)\n",
        "\n",
        "    for verb, bigrams in verb_bigram_dict.items():\n",
        "        for bigram1 in bigrams:\n",
        "                if bigram1 not in G:\n",
        "                  G.add_node(bigram1)\n",
        "                for bigram2 in bigrams:\n",
        "                    if bigram1 != bigram2 and not bigram1[0] in bigram2 and not bigram1[1] in bigram2 and not bigram2[0] in bigram1 and not bigram2[1] in bigram1:\n",
        "                        G.add_node(bigram2)\n",
        "                        if bigram1[0][1] < bigram2[0][1]:\n",
        "                          G.add_edge(bigram1, bigram2, label=verb[0])\n",
        "                        else:\n",
        "                          G.add_edge(bigram2, bigram1, label=verb[0])\n",
        "\n",
        "    return G, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDxygvtUyl_u"
      },
      "outputs": [],
      "source": [
        "# создание графа триграмм\n",
        "def create_dependency_graph_tri(sentence):\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    verb_trigram_dict = {}\n",
        "    embeddings = {}\n",
        "\n",
        "    for sent in doc.sents:\n",
        "\n",
        "      embs = get_bert_vectors(sent.text)\n",
        "\n",
        "      for token in sent:\n",
        "          if token.pos_ == 'VERB':\n",
        "              verb = (token.text, token.i)\n",
        "              if verb not in verb_trigram_dict:\n",
        "                  verb_trigram_dict[verb] = set()\n",
        "\n",
        "              for child in token.children:\n",
        "                  if child.pos_ == 'NOUN' and child.text.isalpha() and len(child.text) > 1:\n",
        "                      noun = (child.text, child.i)\n",
        "                      embeddings_noun = find_embedding(noun[0], embs)\n",
        "\n",
        "                      for grandchild in child.children:\n",
        "                        if grandchild.text.isalpha() and grandchild.pos not in ['VERB', 'DET'] and len(grandchild.text) > 1:\n",
        "                            dependent_word = (grandchild.text, grandchild.i)\n",
        "                            embeddings_dependent_word = find_embedding(dependent_word[0], embs)\n",
        "\n",
        "                            for grandgrandchild in grandchild.children:\n",
        "                                if grandgrandchild.pos not in ['VERB', 'DET', 'ADP'] and grandgrandchild.text.isalpha() and len(grandgrandchild.text) > 1:\n",
        "                                  last_word = (grandgrandchild.text, grandgrandchild.i)\n",
        "                                  embeddings_last_word = find_embedding(last_word[0], embs)\n",
        "\n",
        "                                  trigram = tuple(sorted([noun, dependent_word, last_word], key=lambda x: x[1]))\n",
        "                                  embeddings[trigram] = embeddings_noun + embeddings_dependent_word + embeddings_last_word\n",
        "                                  verb_trigram_dict[verb].add(trigram)\n",
        "\n",
        "    for verb, trigrams in verb_trigram_dict.items():\n",
        "        for trigram1 in trigrams:\n",
        "            G.add_node(trigram1)\n",
        "            for trigram2 in trigrams:\n",
        "                if trigram1 != trigram2 and not trigram1[0] in trigram2 and not trigram1[1] in trigram2 and not trigram1[2] in trigram2 and not trigram2[0] in trigram1 and not trigram2[1] in trigram1 and not trigram2[2] in trigram1:\n",
        "                    G.add_node(trigram2)\n",
        "                    if trigram1[0][1] < trigram2[0][1]:\n",
        "                      G.add_edge(trigram1, trigram2, label=verb[0])\n",
        "                    else:\n",
        "                      G.add_edge(trigram2, trigram1, label=verb[0])\n",
        "\n",
        "    return G, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUSU5bE06tfx"
      },
      "outputs": [],
      "source": [
        "# объединение семантически близких вершин\n",
        "def merge_similar_vertices(graph, embeddings, similarity_threshold):\n",
        "    vertices = list(graph.nodes())\n",
        "    vertex_embeddings = np.array([embeddings[node] for node in vertices])\n",
        "\n",
        "    similarities = cosine_similarity(vertex_embeddings)\n",
        "\n",
        "    for i in range(len(vertices)):\n",
        "        for j in range(i+1, len(vertices)):\n",
        "            if vertices[i] in graph and vertices[j] in graph and vertices[i] in embeddings and vertices[j] in embeddings:\n",
        "              if similarities[i, j] > similarity_threshold:\n",
        "                  #print(vertices[i], vertices[j])\n",
        "                  graph = nx.contracted_nodes(graph, vertices[i], vertices[j], self_loops=False)\n",
        "\n",
        "    return graph"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}