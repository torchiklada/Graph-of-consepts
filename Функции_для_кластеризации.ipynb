{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YUzvR5wC0Hiy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "dpB-ebpydsUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pca(X, num):\n",
        "  scaler = StandardScaler()\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "  pca = PCA(n_components=num)\n",
        "  X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "  return X_pca"
      ],
      "metadata": {
        "id": "Sa0ILojodir_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_verbs_and_embeddings(verbs):\n",
        "    unique_verbs = list(set(verbs))\n",
        "\n",
        "    verb_embeddings = [model[i] for i in unique_verbs]\n",
        "    verb_embeddings = np.array(verb_embeddings)\n",
        "\n",
        "    return unique_verbs, verb_embeddings"
      ],
      "metadata": {
        "id": "xRwR4HEUdtnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(X, n, m, l):\n",
        "  agg_clustering = AgglomerativeClustering(n_clusters=n, metric=m, linkage=l)\n",
        "  cluster_labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "  return cluster_labels"
      ],
      "metadata": {
        "id": "INfC8GhbeD0p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dunn_index(data, labels):\n",
        "    intra_dists = np.zeros(len(np.unique(labels)))\n",
        "\n",
        "    for i, label in enumerate(np.unique(labels)):\n",
        "        cluster_points = data[labels == label]\n",
        "        intra_dists[i] = euclidean_distances(cluster_points).max()\n",
        "\n",
        "    inter_dists = []\n",
        "\n",
        "    for i in range(len(np.unique(labels))):\n",
        "        for j in range(i + 1, len(np.unique(labels))):\n",
        "            cluster_i = data[labels == i]\n",
        "            cluster_j = data[labels == j]\n",
        "            inter_dists.append(euclidean_distances(cluster_i, cluster_j).min())\n",
        "\n",
        "    dunn_index = np.min(inter_dists) / np.max(intra_dists)\n",
        "\n",
        "    return dunn_index\n",
        "\n",
        "def xie_beni_index(data, labels):\n",
        "    centroids = np.array([data[labels == label].mean(axis=0) for label in np.unique(labels)])\n",
        "    intra_dists = []\n",
        "\n",
        "    for i, label in enumerate(np.unique(labels)):\n",
        "        cluster_points = data[labels == label]\n",
        "        intra_dists.append(np.mean(euclidean_distances(cluster_points, [centroids[i]])))\n",
        "\n",
        "    min_inter_dist = np.min(euclidean_distances(centroids, centroids))\n",
        "\n",
        "    xie_beni_index = np.sum(intra_dists) / (len(data) * min_inter_dist)\n",
        "\n",
        "    return xie_beni_index"
      ],
      "metadata": {
        "id": "Wh19F1ExeSk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_parameters(verbs, verb_embeddings):\n",
        "    n_clusters = [i for i in range(5, 16)]\n",
        "    metric = ['euclidean', 'cosine', 'manhattan']\n",
        "    linkage = ['ward', 'complete', 'average', 'single']\n",
        "    n_pca = [i for i in range(10, 231, 10)]\n",
        "\n",
        "    max_score = -np.inf\n",
        "    ans = ''\n",
        "\n",
        "    for i in n_pca:\n",
        "      for j in n_clusters:\n",
        "        for k in metric:\n",
        "          for l in linkage:\n",
        "            data = pca(verb_embeddings, i)\n",
        "\n",
        "            try:\n",
        "              labels = get_labels(verb_embeddings, j, k, l)\n",
        "\n",
        "              ch_score = metrics.calinski_harabasz_score(data, labels)\n",
        "              silhouette_score = metrics.silhouette_score(data, labels)\n",
        "              db_score = metrics.davies_bouldin_score(data, labels)\n",
        "              dunn_score = dunn_index(data, labels)\n",
        "              xb_score = xie_beni_index(data, labels)\n",
        "\n",
        "              metrics_array = np.array([ch_score, silhouette_score, db_score, dunn_score, xb_score])\n",
        "\n",
        "              metrics_array[~np.isfinite(metrics_array)] = np.nanmax(metrics_array[np.isfinite(metrics_array)])\n",
        "              scaler = StandardScaler()\n",
        "              normalized_metrics = scaler.fit_transform(metrics_array.reshape(-1, 1))\n",
        "\n",
        "              combined_score = np.mean(normalized_metrics)\n",
        "\n",
        "              if combined_score > max_score:\n",
        "                max_score = combined_score\n",
        "                ans = (i, j, k, l)\n",
        "\n",
        "            except:\n",
        "              pass\n",
        "\n",
        "    return ans"
      ],
      "metadata": {
        "id": "McH-xkJ3e-eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def get_cluster_info(cluster_labels, words, embeddings):\n",
        "\n",
        "    unique_labels = np.unique(cluster_labels)\n",
        "\n",
        "    cluster_info = {}\n",
        "\n",
        "    for label in unique_labels:\n",
        "        cluster_indices = np.where(cluster_labels == label)[0]\n",
        "        cluster_words = [words[i] for i in cluster_indices]\n",
        "        cluster_embeddings = embeddings[cluster_indices]\n",
        "\n",
        "        distance_matrix = euclidean_distances(cluster_embeddings)\n",
        "        mean_distances = np.mean(distance_matrix, axis=1)\n",
        "\n",
        "        centroid_index = np.argmin(mean_distances)\n",
        "        centroid_word = cluster_words[centroid_index]\n",
        "\n",
        "        cluster_info[label] = {\n",
        "            'words': cluster_words,\n",
        "            'centroid': centroid_word,\n",
        "        }\n",
        "\n",
        "    return cluster_info"
      ],
      "metadata": {
        "id": "x-SJJRm5m7u3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}